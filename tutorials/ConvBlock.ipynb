{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from convblock import ConvBlock, ConvBranches, Config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create simple 2D convolutional block consisting of two convolutions followed by **batch norm** and **relu** activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvBlock(\n",
       "  (Module_0): Conv(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "  (Module_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Module_2): ReLU(inplace=True)\n",
       "  (Module_3): Conv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "  (Module_4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Module_5): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32), layout='cna cna', c=dict(kernel_size=[3, 3], filters=[16, 16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input shape is provided one time for each block, number of output channels and padding values are automaticallyt computed during **ConvBlock** construction. **ConvBlock** has following properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = ConvBlock(input_shape=(32, 32, 32), layout='cna cna', c=dict(kernel_size=[3, 3], stride=[1, 2], filters=[16, 16]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2.0, 2.0), array([32, 32, 32]), array([16, 16, 16]), 32, 16)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.stride, block.input_shape, block.output_shape, block.in_channels, block.out_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say, for trasposed convolution **stride** will be 0.5 across two dimensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "block = ConvBlock(input_shape=(32, 32, 32), layout='cna t',\n",
    "                  c=dict(kernel_size=3, stride=1, filters=16),\n",
    "                  t=dict(kernel_size=3, stride=2, filters=17))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.5, 0.5), array([32, 32, 32]), array([17, 64, 64]), 32, 17)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block.stride, block.input_shape, block.output_shape, block.in_channels, block.out_channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each basic typical operation like *convolution*, *pooling*, *dropout*, *batchnorm*, *instancenorm*, *activation* and etc there is a one-char shortcut, that can be specified in layout. In example above layout can be viewed as a simple sequence of **convolution**, **batch normalization** and **activation**. Note that default value for activation is 'relu'. If one would like to change **batch normalization** to **instance normalization** without affine parameters, it would be as simple as following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvBlock(\n",
       "  (Module_0): Conv(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "  (Module_1): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (Module_2): ReLU(inplace=True)\n",
       "  (Module_3): Conv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "  (Module_4): InstanceNorm2d(16, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "  (Module_5): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32), layout='cia cia', i=dict(affine=False), c=dict(kernel_size=[3, 3], filters=[16, 16]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see affine parameter was set to **False** for all **instance normalization** operations in block. This automatic vectorization of parameters of layout on par with spatial unsqueezing(more on that latter) allow us to simplify convolutional block design."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get list of all avaliable options of layout using folowing code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'shortcut': <function convblock.pytorch.layers.conv_block.res_shortcut(input_shape, output_shape, layout='cna', kernel_size=1, stride=1, dilation=1, groups=1, bias=False, pool_size=2, pool_mode='max', allow_identity=True, broadcast=True, mode='+', filters=None, downsample_mode='c', **kwargs)>,\n",
       " 'c': convblock.pytorch.layers.conv.Conv,\n",
       " 't': convblock.pytorch.layers.conv.ConvTransposed,\n",
       " '<': convblock.pytorch.layers.layers.Flatten,\n",
       " 'u': convblock.pytorch.layers.layers.Upsample,\n",
       " 'n': convblock.pytorch.layers.layers.BatchNorm,\n",
       " 'i': convblock.pytorch.layers.layers.InstanceNorm,\n",
       " 'f': convblock.pytorch.layers.layers.Linear,\n",
       " 'l': convblock.pytorch.layers.layers.Lambda,\n",
       " 's': convblock.pytorch.layers.layers.ChannelsShuffle,\n",
       " 'd': convblock.pytorch.layers.layers.Dropout,\n",
       " 'p': <function convblock.pytorch.layers.pool.Pool(input_shape, kernel_size=3, stride=2, dilation=1, mode='max', padding='constant', norm_type=1.0, output_size=None)>,\n",
       " '>': convblock.pytorch.layers.pool.GlobalPool,\n",
       " 'g': convblock.pytorch.layers.pool.GlobalPool,\n",
       " 'a': convblock.pytorch.layers.activation.Activation}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock.get_options()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that most of them are quite intuitive to understand except maybe **shortcut** option that is the special option and is not one-char. More on shortcuts you can find in **Residual Connections** part of tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic parameter vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ConvBlock** inner design allows you not to duplicate parameters of the same type operations provided in layout in case they are the same for all operations of that type. Also some operations have default values for their parameters(stride=1 for convolution operation 'c' layout option) and activation='relu' for activationn ('a' layout option). So in case of our simple block you may avoid providing filters for both convolutions and set in simply to 16. **ConvBlock** will automatically detect number of convolutions in layout and duplicate the parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvBlock(\n",
       "  (Module_0): Conv(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "  (Module_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Module_2): ReLU(inplace=True)\n",
       "  (Module_3): Conv(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "  (Module_4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Module_5): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32), layout='cna cna', c=dict(kernel_size=[3, 3], filters=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use (1, 3) => (3, 1) stack of convolutions instead(like it's implemented in **inception** architecture) it would be as simple ass following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvBlock(\n",
       "  (Module_0): Conv(32, 16, kernel_size=(1, 3), stride=(1, 1), padding=(0, 0, 1, 1), padding_mode=constant, bias=False)\n",
       "  (Module_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Module_2): ReLU(inplace=True)\n",
       "  (Module_3): Conv(16, 16, kernel_size=(3, 1), stride=(1, 1), padding=(1, 1, 0, 0), padding_mode=constant, bias=False)\n",
       "  (Module_4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (Module_5): ReLU(inplace=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32), layout='cna cna', c=dict(kernel_size=[(1, 3), (3, 1)], filters=16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that padding is automatically computed for convolutions and pooling operations to emulated the 'SAME' mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Residual Connections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three types of residual connections implemented in library: add, concat and mul. Each of residual connection type requires specification of starting and end point of residual connection by corresponding shortcut(+ for add, * for mul and . for concat). So, let's add residual connection to our simple block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32), layout='+ cna cna +', c=dict(kernel_size=3, filters=[16, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the same concept of residual connection as used in **ResNet** paper: if output shape does not match input shape then use 1x1 convolution otherwise use raw input. Of course there is a way to customize behaviour shortcut  in **ConvBlock**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(32, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (Module_0): Conv(32, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32),\n",
    "          layout='+ cna cna +',\n",
    "          c=dict(kernel_size=3, filters=[16, 32]),\n",
    "          shortcut={'allow_identity': False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, **ConvBlock** was forced to use shorcut convolution by setting **allow_identity=False**. One can also customized the layout of shortcut layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "        (Module_6): Conv(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_8): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (Module_0): Conv(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32),\n",
    "          layout='+ cna cna cn +',\n",
    "          c=dict(kernel_size=[1, 3, 1], filters=[8, 8, 32], stride=[1, 2, 1]),\n",
    "          shortcut=dict(layout='c', kernel_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding post activation is really easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(8, 8, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "        (Module_6): Conv(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): ConvBlock(\n",
       "        (Module_0): Conv(32, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): ConvBlock(\n",
       "    (Module_0): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32),\n",
    "          layout='+ cna cna cn + a',\n",
    "          c=dict(kernel_size=[1, 3, 1], filters=[8, 8, 32], stride=[1, 2, 1]),\n",
    "          shortcut=dict(layout='c', kernel_size=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortcut convolution was set to use 3x3 kernel and we changed layout to 'c' insted of default 'cna'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvBlock allows to use several shortcut connections in one block followed one by one. You can also combine different types of residual connections. Let's suppose that you want to create a more complicated block with 'concat' residual connection similiar to one used in **DenseNet**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (1): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(64, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (2): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(96, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (3): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): ConvBlock(\n",
       "        (Module_0): Conv(128, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_2): ReLU(inplace=True)\n",
       "        (Module_3): Conv(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "        (Module_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (Module_5): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32),\n",
    "          layout='. cna cna .' * 4,\n",
    "          c=dict(kernel_size=[1, 3] * 4,\n",
    "                 filters=[8, 32] * 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual in Residual pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): ConvBlock(\n",
       "    (Module_0): Conv(32, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2, 2, 2), padding_mode=constant, bias=False)\n",
       "    (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (Module_2): ReLU(inplace=True)\n",
       "  )\n",
       "  (1): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): Branches(\n",
       "          (branches): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (Module_0): Conv(8, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (Module_2): ReLU(inplace=True)\n",
       "              (Module_3): Conv(8, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "              (Module_4): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "            (1): ConvBlock(\n",
       "              (Module_0): Conv(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (Module_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (Module_2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (1): ConvBlock(\n",
       "          (Module_0): Conv(32, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2, 2, 2), padding_mode=constant, bias=False)\n",
       "          (Module_1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (2): ConvBlock(\n",
       "    (Module_0): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32),\n",
    "          layout='cna . + cna cn + cn . a',\n",
    "          c=dict(kernel_size=[5, 1, 3, 5],\n",
    "                 filters=[8, 8, 32, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently **ConvBlock** does not support residual in residual pattern for the same residual type because of the design restriction. + ... + ... + ... + pattern will be interpreted as sequence of two residuals followed one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finnaly, let's create a ResBlock with SE block for self attention:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Branches(\n",
       "    (branches): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (0): ConvBlock(\n",
       "          (Module_0): Conv(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (Module_1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Module_2): ReLU(inplace=True)\n",
       "          (Module_3): Conv(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1, 1, 1), padding_mode=constant, bias=False)\n",
       "          (Module_4): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          (Module_5): ReLU(inplace=True)\n",
       "          (Module_6): Conv(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (Module_7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "        (1): Branches(\n",
       "          (branches): ModuleList(\n",
       "            (0): ConvBlock(\n",
       "              (Module_0): AdaptiveAvgPool(input_shape=[32 32 32], output_shape=[32  1  1])\n",
       "              (Module_1): Conv(32, 8, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (Module_2): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (Module_3): ReLU(inplace=True)\n",
       "              (Module_4): Conv(8, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "              (Module_5): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "              (Module_6): Sigmoid()\n",
       "            )\n",
       "            (1): Identity()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): Identity()\n",
       "    )\n",
       "  )\n",
       "  (1): ConvBlock(\n",
       "    (Module_0): ReLU(inplace=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ConvBlock(input_shape=(32, 32, 32),\n",
    "          layout='+ cna cna cn * p cna cna * + a',\n",
    "          p=dict(mode='avg', output_size=1),\n",
    "          a=dict(activation=['relu', 'relu', 'relu', 'sigmoid', 'relu']),\n",
    "          c=dict(kernel_size=[1, 3, 1, 1, 1], filters=[8, 8, 32, 8, 32]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **ConvBlock** allows you to design complex convolutional modules with non-linear sequential sturcture using just a few lines of code. In second tutorial you will learn to use **ConvBranches** module that allow to create even more complex structures with several branches."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
